# -*- coding: utf-8 -*-
"""fine-tune.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E7esMuOS6yV-3lmhSiHwa8Bbw45lM7xL
"""

# !pip install -q datasets transformers accelerate torchaudio librosa

import json
import numpy as np
import torch
from datasets import Dataset, Audio
from transformers import (
    WhisperProcessor,
    WhisperForConditionalGeneration,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
)

# ✅ Load JSONL manually (avoiding filesystem error)
with open("dataset.jsonl", "r") as f:
    data = [json.loads(line) for line in f]

dataset = Dataset.from_list(data)
dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))

# ✅ Load Whisper processor and model
model_name = "openai/whisper-small"
processor = WhisperProcessor.from_pretrained(model_name)
model = WhisperForConditionalGeneration.from_pretrained(model_name)

# ✅ Preprocessing: pad or truncate to 3000 frames
def preprocess(batch):
    audio = batch["audio"]
    mel = processor.feature_extractor(
        audio["array"],
        sampling_rate=audio["sampling_rate"],
        return_tensors="np"
    )["input_features"][0]  # (80, T)

    desired_len = 3000
    current_len = mel.shape[1]
    if current_len < desired_len:
        mel = np.pad(mel, ((0, 0), (0, desired_len - current_len)), mode="constant")
    else:
        mel = mel[:, :desired_len]

    batch["input_features"] = torch.tensor(mel, dtype=torch.float32)

    # ✅ Truncate labels to 448 tokens
    tokens = processor.tokenizer(batch["transcription"]).input_ids
    batch["labels"] = tokens[:448]
    return batch

dataset = dataset.map(preprocess)

# ✅ Custom data collator
class WhisperDataCollator:
    def __call__(self, features):
        input_features = [torch.tensor(f["input_features"]) if not isinstance(f["input_features"], torch.Tensor) else f["input_features"] for f in features]
        labels = [torch.tensor(f["labels"]) for f in features]

        return {
            "input_features": torch.stack(input_features),
            "labels": torch.nn.utils.rnn.pad_sequence(
                labels, batch_first=True, padding_value=-100
            ),
        }

# ✅ Training args
training_args = Seq2SeqTrainingArguments(
    output_dir="./whisper-finetuned",
    per_device_train_batch_size=1,
    learning_rate=1e-5,
    num_train_epochs=5,
    logging_dir="./logs",
    logging_steps=1,
    save_strategy="epoch",
    fp16=False,
)

# ✅ Trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    data_collator=WhisperDataCollator(),
    tokenizer=processor,  # use the full processor here
)

# ✅ Train
trainer.train()

# !pip install -q transformers torchaudio librosa jiwer

import os
import torch
import torchaudio
from jiwer import wer
from transformers import WhisperProcessor, WhisperForConditionalGeneration, AutoProcessor

# Load original processor and fine-tuned model
processor = AutoProcessor.from_pretrained("openai/whisper-small")
model = WhisperForConditionalGeneration.from_pretrained("./whisper-finetuned/checkpoint-10")
model.generation_config.forced_decoder_ids = None
model.eval()

# ✅ Directory where test audio files (.wav) are stored
audio_dir = "tests"
os.makedirs(audio_dir, exist_ok=True)

# ✅ Transcribe each audio file
results = []
for file_name in sorted(os.listdir(audio_dir)):
    if not file_name.endswith(".wav"):
        continue

    file_path = os.path.join(audio_dir, file_name)
    speech_array, sr = torchaudio.load(file_path)
    if sr != 16000:
        speech_array = torchaudio.transforms.Resample(sr, 16000)(speech_array)

    inputs = processor(
        speech_array.squeeze().numpy(),
        sampling_rate=16000,
        return_tensors="pt",
        return_attention_mask=True
    )
    inputs["decoder_input_ids"] = torch.tensor([[50259]])  # 'en' token for Whisper

    # Then generate as usual
    with torch.no_grad():
        predicted_ids = model.generate(
            inputs["input_features"],
            max_length=448
        )
        transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]

    results.append((file_name, transcription))
    print(f"{file_name} → {transcription}")

output_path = "inference_results.txt"
with open(output_path, "w") as f:
    for fname, text in results:
        f.write(f"{fname}\t{text.strip()}\n")

print(f"\n✅ Transcriptions saved to: {output_path}")

from jiwer import wer

# Load predictions
with open("inference_results.txt", "r") as f:
    pred_dict = dict(line.strip().split("\t", 1) for line in f if line.strip())

# Load ground truth
with open("ground_truth.txt", "r") as f:
    truth_dict = dict(line.strip().split("\t", 1) for line in f if line.strip())

# Compute WER per file
total_wer = 0
for fname in pred_dict:
    pred = pred_dict[fname].lower()
    truth = truth_dict.get(fname, "").lower()
    file_wer = wer(truth, pred)
    total_wer += file_wer
    print(f"{fname}: WER = {file_wer:.3f}")

# Average WER
avg_wer = total_wer / len(pred_dict)
print(f"\nAverage WER: {avg_wer:.3f}")